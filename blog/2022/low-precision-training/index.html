<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Jiawei Zhao | Low-Precision Training with Multiplicative Weight Update Algorithm Madam</title>
    <meta name="author" content="You r. Name" />
    <meta name="description" content="none" />
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website" />


    <!-- Bootstrap & MDB -->
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://jiaweizzhao.github.io/blog/2022/low-precision-training/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://jiaweizzhao.github.io/">Jiawei Zhao</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/publications/">publications</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/projects/">projects</a>
                </div>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Low-Precision Training with Multiplicative Weight Update Algorithm Madam</h1>
    <p class="post-meta">January 10, 2022</p>
    <p class="post-tags">
      <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>
      Â  Â· Â 
        <a href="/blog/category/projects">
          <i class="fas fa-tag fa-sm"></i> projects</a> Â 
          

    </p>
  </header>

  <article class="post-content">
    <!-- ### Abstract -->

<!-- Representing deep neural networks (DNNs) in low-precision is a promising approach to enable efficient acceleration and memory reduction. However, directly training DNNs with low-precision weights leads to accuracy degradation due to complex interactions between the low-precision number systems and the learning algorithms. To address this issue, we develop a co-designed low-precision training framework, termed LNS-Madam, in which we jointly design a logarithmic number system (LNS) and a multiplicative weight update algorithm (Madam). Compared to a full-precision floating-point implementation, 8-bit LNS-Madam reduces the energy consumption by over 90% while preserving the prediction performance. -->

<!-- ### Background -->

<p>Training deep neural networks consumes extensive energy and generates a large amount of carbon emissions. For example, suggested by <a href="https://www.techtarget.com/searchenterpriseai/feature/Energy-consumption-of-AI-poses-environmental-problems" target="_blank" rel="noopener noreferrer">this article</a>, training a final version of MegatronLM costs 27,648 kilowatt hours (kWh), which is similar to the energy consumption of three households in the U.S. for a year. Traditionally, values in neural networks are represented using floating-point (32-bit) numbers, which incurs large arithmetic and memory footprint, and hence significant energy consumption. In contrast, low-precision numbers only require low-bitwidth computational units, leading to better computational efficiency and less required memory bandwidth and capacity. To achieve low-precision training, we develop a co-designed low-precision training framework, termed LNS-Madam, in which we jointly design a logarithmic number system (LNS) and a multiplicative weight update algorithm (Madam).</p>

<h3 id="list-of-papers">List of Papers</h3>

<p><strong>Learning compositional functions via multiplicative weight updates <a href="https://arxiv.org/abs/2006.14560" target="_blank" rel="noopener noreferrer">(link)</a></strong>
<!-- Jeremy Bernstein, Jiawei Zhao, Markus Meister, Ming-Yu Liu, Anima Anandkumar, Yisong Yue --></p>

<p><strong>LNS-Madam: Low-Precision Training in Logarithmic Number System using Multiplicative Weight Update <a href="https://arxiv.org/abs/2106.13914" target="_blank" rel="noopener noreferrer">(link)</a></strong>
<!-- Jiawei Zhao, Steve Dai, Rangharajan Venkatesan, Ming-Yu Liu, Brucek Khailany, Bill Dally, Anima Anandkumar --></p>

<h3 id="madam-optimizer---multiplicative-weight-update-for-deep-learning">Madam Optimizer - Multiplicative Weight Update for Deep Learning</h3>

<p>Learning compositional functions via gradient descent incurs well known problems like vanishing and exploding gradients, making careful learning rate tuning essential for real-world applications. In contrast to additive learning algorithms such as gradient descent, we propose a multiplicative weight update method Madam - a multiplicative version of the Adam optimizer. Madam updates the weights directly in logarithmic space, such that:</p>

\[W \gets W \odot \exp \left[-\eta\,\mathrm{sign} W \odot\, \left(\frac{g}{\bar{g}}\right)\right]\]

<p>where \(\odot\) denotes element-wise multiplication, \(\eta\) is the learning rate, and \(\bar{g}\) represent the normalized gradient.</p>

<div class="col-sm mt-3 mt-md-0 mx-auto">
    <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/lr-compare-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/lr-compare-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/lr-compare-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/lr-compare.jpg">
  </picture>

</figure>

</div>

<p>We prove that multiplicative weight updates satisfy a descent lemma tailored to compositional functions. As shown above, our empirical results suggest that Madam can train state-of-the-art neural network architectures without learning rate tuning.</p>

<h3 id="representing-neural-networks-in-logarithmic-number-system">Representing Neural Networks in Logarithmic Number System</h3>

<div class="col-sm mt-3 mt-md-0 mx-auto">
    <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/lns_synaptic-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/lns_synaptic-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/lns_synaptic-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/lns_synaptic.png">
  </picture>

</figure>

</div>

<p>While low-precision training methods generally reduce computational costs, energy efficiency can be further improved by choosing a logarithmic number system (LNS) for representing numbers. As shown in the above figure (left)ï¼Œ we compare bfloat16 number system (upper) with a logarithmic number system (lower), where LNS only has sign and exponents. Representing neural networks using LNS is in line with biological findings, suggested by <a href="https://elifesciences.org/articles/10778" target="_blank" rel="noopener noreferrer">Bartol et al.</a>, that the brain may use ``a form of non-uniform quantization which efficiently encodes the dynamic range of synaptic strengths at constant precisionâ€™â€™â€”or in other words, a logarithmic number system. The above figure (right) is the synaptic distribution suggested by <a href="https://elifesciences.org/articles/10778" target="_blank" rel="noopener noreferrer">Bartol et al.</a>.</p>

<p>In addition, LNS is energy efficient. It achieves a higher computational efficiency by transforming expensive multiplication operations in the network layers to inexpensive additions in their logarithmic representations.</p>

<h3 id="lns-madam">LNS-Madam</h3>

<div class="col-sm mt-3 mt-md-0 mx-auto" style="max-width: 400px;">
    <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/add_vs_mul-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/add_vs_mul-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/add_vs_mul-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/add_vs_mul.jpg">
  </picture>

</figure>

</div>

<p>Because Madam optimizer directly updates the weights in the logarithmic space, it is natural to apply Madam over the weights in LNS. As shown above, Madam induces less quantization error compared to gradient descent, as Madamâ€™s updates are scaled with the weight magnitudes accordingly. Motivated by this benefit, we develop a co-designed low-precision training framework LNS-Madam, in which we jointly design the logarithmic number system (LNS) and the multiplicative weight update algorithm Madam.</p>

<h3 id="results">Results</h3>

<div class="col-sm mt-3 mt-md-0 mx-auto">
    <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/LNS-Madam-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/LNS-Madam-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/LNS-Madam-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/LNS-Madam.jpg">
  </picture>

</figure>

</div>

<p>We compare Madam with different optimizers over various datasets and models. As shown above, training with Madam leads to a stable convergence even if precision is strongly limited.</p>

<div class="col-sm mt-3 mt-md-0 mx-auto" style="max-width: 400px;">
    <figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/energy_results-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/energy_results-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/energy_results-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid" src="/assets/img/energy_results.jpg">
  </picture>

</figure>

</div>

<p>8-bit LNS-Madam also shows great energy efficiency. Compared to a full-precision floating-point implementation, 8-bit LNS-Madam reduces the energy consumption by over 90% while preserving the prediction performance.</p>

  </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2022 You r. Name. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>
  </body>

  <!-- jQuery -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  <!-- Mansory & imagesLoaded -->
  <script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  <script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
  
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

  
</html>

